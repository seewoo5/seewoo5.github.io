---
layout: posts
title:  "ICML 2023 Review"
date:   2023-07-30
categories: jekyll update
tags: machine-learning
---

I just returned from Honolulu, Hawaii, where this year's ICML was held.
Although I do not major in ML, I attended to present my work on homomorphic encryption-based transfer learning that was done while I was at CryptoLab.
As a mathematician, most of the buzzwords were new to me, so I had to ask the authors or other friends for the definitions of the words I hadn't seen before to understand what the papers were doing.
Anyway, I met many of my old friends and some new people, and it was good to know what they were working on.
There were over 7000 people at the conference, which was huge compared to any of the math conferences (maybe ICM or JMM have that many attendees).
Understanding everything at the conference was impossible, so I needed choice and focus.
I tried to attend all of my friends' posters (with many stupid questions) and also visited the ones with interesting titles (especially math-ey ones).
I'll explain some of the interesting works that I've seen at the conference in my own words, which could be wrong.
Let me know if there are any problems with my explanations.
To read the papers, click the titles.


<p align="center">
<img src="/assets/images/icml2023-nametag.png">
<figcaption align="center">My nametag</figcaption>
</p>

## My favorite papers at ICML


### [HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic Encryption](https://proceedings.mlr.press/v202/lee23m/lee23m.pdf)

This is our (CryptoLab) work that is accepted as an oral presentation.


### [SpENCNN: Orchestrating Encoding and Sparsity for Fast Homomorphically Encrypted Neural Network Inference](https://proceedings.mlr.press/v202/ran23b/ran23b.pdf)

There were only **TWO** papers on homomorphic encryption at ICML (among 1827 accepted papers), and this paper was a unique work on HE that was not ours. 
The paper is about the encrypted inference of CNN, which is one of the widely studied topics in HE-based PPML.
The primary goal is to reduce the number of rotations, which is the most costly operation in CNN inference (other than bootstrapping).
One of the most recent work on this topic is [Lee et al., Low-Complexity Deep Convolutional Neural Networks on Fully Homomorphic Encryption Using Multiplexed Parallel Convolutions](https://proceedings.mlr.press/v162/lee22e/lee22e.pdf), which suggests *Multiplexed Parallel Convolutions* that compactly pack the multiple channel's weight into single (or few) ciphertexts.
(Another contribution of the paper is the *imaginary-removing bootstrapping* that handles catastrophic divergence of approximate RELU operations, but here I only concentrate on reducing computational costs of CNN layers.)

HE-based CNN inference consists of two types of rotations - *inner* rotations and *outer* rotations.
Each input channel's ciphertext needs to be rotated $K^2 - 1$ times (for the kernel size of $K$), and these rotations are called inner rotations.
After the (inner-)rotated input channels are multiplied with (plaintext) weights, these will be summed up and result in an intermediate ciphertext for each input channel.
These results are concatenated into a single ciphertext, and the outer rotations rotate the concatenated ciphertext, and output feature maps are obtained by summing up all the (outer-)rotated ciphertexts.
Since each outer rotation introduces $K^2 - 1$ inner rotations, it is important to reduce the number of outer rotations.

In this paper, they develop **HE-group convolution** and associated group-interleaved encoding to optimize channel locations on ciphertexts based on the number of convolutional groups and ciphertext size (Figure (a) below).
To do this, we must replace the traditional row-major encoding with the group-interleaved encoding (Figure (b) below).
This significantly reduces the number of outer rotations.

<p align="center">
<img src="/assets/images/icml2023-spencnn-hegroup.png">
<figcaption align="center">HE-group convolution and group-interleaved encoding, Ran et al. 2023</figcaption>
</p>


Another contribution of the paper is the **Sub-block Pruning**, which reduces the number of inner rotations by pruning weights that correspond to specific inner rotations.
They defined the weight importance of the weight $w_{p_i}$ corresponds to a given sub-block $p_i$ as $\|\|w_{p_i}\|\| / \dim (w_{p_i})$ (where the norm is $L^2$-norm) and did pruning-and-retraining until one can recover the accuracy of the original (non-pruned) model.
Note that the existing pruning methods (e.g. [Han et al.](https://arxiv.org/pdf/1510.00149.pdf)) may not reduce the number of rotations at all; hence we need HE-packing-aware pruning methods as suggested to reduce computational costs.
There's a previous work called [Hunter](https://dl.acm.org/doi/10.1145/3488932.3517401) which has a similar approach to pruning, but SpENCNN was much more effective in terms of sparsity, accuracy, and latency (Table 5).

<p align="center">
<img src="/assets/images/icml2023-spencnn-sparse.png">
<figcaption align="center">Weight sparcity of convolutional layers and FC layers in SpENCNN, Ran et al. 2023</figcaption>
</p>

<p align="center">
<img src="/assets/images/icml2023-spencnn-table5.png">
<figcaption align="center">Comparison with Hunter, Ran et al. 2023</figcaption>
</p>

The following figure summarizes the overall process of SpENCNN.
First, we find the optimal group number $G$ and use HE-group convolution with $G$ groups.
Then we use sub-block pruning to reduce the number of inner rotations further.
This results in a pruned CNN model with almost the same accuracy but with fewer inference costs.

<p align="center">
<img src="/assets/images/icml2023-spencnn.png">
<figcaption align="center">Overall process of SpENCNN, Ran et al. 2023</figcaption>
</p>

### [Effectively Using Public Data in Privacy Preserving Machine Learning](https://proceedings.mlr.press/v202/nasr23a/nasr23a.pdf)

### [Leveraging Proxy of Training Data for Test-Time Adaptation](https://proceedings.mlr.press/v202/kang23a/kang23a.pdf)


### [Modality-Agnostic Variational Compression of Implicit Neural Representations](https://proceedings.mlr.press/v202/schwarz23a/schwarz23a.pdf)


### [Diffusion Probabilistic Models Generalize when They Fail to Memorize](https://openreview.net/pdf?id=shciCbSk9h)

### [A Watermark for Large Language Models](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf)

This paper is already reviewed in the [previous post](https://seewoo5.github.io/jekyll/update/2023/01/28/chatgpt-watermark.html).
Note that the paper is one of the [outstanding paper award](https://icml.cc/Conferences/2023/Awards) recipients of this year's ICML.


### [Cross-Entropy Loss Functions: Theoretical Analysis and Applications](https://proceedings.mlr.press/v202/mao23b/mao23b.pdf)

### [Learning-Rate-Free Learning by D-Adaptation](https://proceedings.mlr.press/v202/defazio23a/defazio23a.pdf)

### [Scaling Vision Transformers to 22 Billion Parameters](https://proceedings.mlr.press/v202/dehghani23a/dehghani23a.pdf)

### [Fast Inference from Transformers via Speculative Decoding](https://proceedings.mlr.press/v202/leviathan23a/leviathan23a.pdf)

### List of other interesting papers that I can't review since I'm lazy

Here is a list of papers that looks interesting, but I cannot review them since I have yet to read them thoroughly, and I'm too lazy to read them.
I may write separate posts once I read them and find them attractive.

* [SparseProp: Efficient Sparse Backpropagation for Faster Training of Neural Networks at the Edge](https://proceedings.mlr.press/v202/nikdan23a/nikdan23a.pdf)
* [Resurrecting Recurrent Neural Networks for Long Sequences](https://proceedings.mlr.press/v202/orvieto23a/orvieto23a.pdf)
* [Architecture-Agnostic Masked Image Modeling â€“ From ViT back to CNN](https://proceedings.mlr.press/v202/li23af/li23af.pdf)
* [Convex Geometry of ReLU-Layers, Injectivity on the Ball and Local Reconstruction](https://proceedings.mlr.press/v202/haider23a/haider23a.pdf)
* [Hyperbolic Representation Learning: Revisiting and Advancing](https://proceedings.mlr.press/v202/yang23u/yang23u.pdf)



## Other events at ICML (and Honolulu)


There were many events at ICML, especially social events.
Since I'm neither ML nor a social person, I didn't attend many of the events, but I've been to two of them - ML in Korea and Jane Street lunch.
Obviously, ML in Korea is for Korean ML researchers, hosted by Naver and sponsored by other AI startups.
There were over 200 people and very crowded.
The seats were divided into groups based on their research interests like NLP, Diffusion models, Vision, etc.
Sadly, there was no *privacy* section, so I sat on some random seat next to my coauthor.
The hosts gave some quizzes with prizes, and the first quizzes were doing human MLM with paper titles, i.e., fill the blanks in the title of the given paper presented at this year's ICML.
I know some papers (Scaling ViT to 22B and DetectGPT), but most of them are the ones I had never seen before.
OX quizzes were much harder, but my random number generator worked pretty well. Eventually, I got the following prize at the event (this is a masking tape with some character that represents a tired graduate student).

<p align="center">
<img src="/assets/images/icml2023-mlkorea.png">
<figcaption align="center">Prize from ML in Korea.</figcaption>
</p>

For the Jane Street lunch, I got an invitation email about it, and honestly, I had no idea why they invited me (they may know my email since I applied to their fellowship last year, and also, they figured out that I'd participate in ICML, but how?)
Anyway, I met some Jane Street people there, and it was pretty fun (and the food was also nice).
But honestly, the conversation with a random non-Jane Street person who is working at IBM Tokyo and had an experience with HE was much more interesting.
I also attended his poster session on molecule stuff (which is introduced above).


But, the most important point of this year's ICML is that it was held in Hawaii (which also explains why there were so many attendees).
Although I'm not an outgoing person, I did turtle snorkeling and also visited some beaches near the conference center and the hotel.
I also visited HoMA, Honolulu Museum of Arts, which exhibits a wide range of arts in various regions, including Honolulu, Egypt, Indonesia, Japan, China, and Korea.
The museum was quite big, and it took almost 2 hours to visit all the places.

<p align="center">
<img src="/assets/images/icml2023-turtle.jpeg">
<figcaption align="center">Sea turtle.</figcaption>
</p>

<p align="center">
<img src="/assets/images/icml2023-homa.jpeg">
<figcaption align="center">"The Long Rain" by Yves Tanguy at HoMA. This painting was interesting because it looks like computer art but is actually an oil painting.</figcaption>
</p>


I didn't attend the tutorials and workshops since I did not know about the topics.
Also, I misunderstood that the tutorials were doing some live coding by following instructions, but they were just another type of workshop, eventually explaining the organizer's work on the subject.
But now I regret it a bit since I could attend these events and learn some ongoing ML trends.