---
layout: posts
title:  "LoRA: Low-Rank Adaptation of Large Language Models"
date:   2023-05-02
categories: jekyll update
tags: machine-learning
---


Large language models have become ubiquitous, with their development and deployment having transformed the field of artificial intelligence in recent years.
These models, such as GPT-4, have been trained on massive amounts of data and are capable of understanding and generating human-like language with a high degree of accuracy.
They have been used in a variety of applications, from language translation to chatbots and voice assistants.


## Fine-tuning LLMs efficiently

However, since they are large, they require a lot of resources to train, which cannot be done by individuals. Therefore, one may fine-tune the models for their specific purpose or add adapters to make them work well on downstream tasks while keeping the additional training efficient.
However, there are some issues with existing methods.


### Introduce inference latency by extending model depth

[Houlsby et al.] and [Lin et al.] add some adaptive layers (two adapter layers per Transformer block or one per block + additional LayerNorm), which obviously introduces additional computations.
Some of the later works ([Rücklé et al.] and [Pfeiffer et al.]) reduces the overall latency by applying pruning, but this can't bypass the extra computation from adaptive layers.
Although they add only small number of parameters (less than 1% compared to the original model), adding such adaptation layers can introduces bottlenecks for hardware parallelization.
The below table (Table 1) shows the increase in latency when using the above adapters, which is more than 20% to 30% in some cases.
Note that the problem worsens when we need to shard the model because the additional depth requires more synchronous GPU operations, unless we store the adapter parameters redundantly multiple times.


<p align="center">
<img src="/assets/images/lora-table1.png">
</p>


### Reduce the model's usuable sequence length

[Li and Liang] proposes

### Fail to match the fine-tuning baselines

At last, most of the previous works often fail to match the performances of the full fine-tuning baselines.


## LoRA

In this post, we introduce LoRA, a method to improve the performance of LLMs on downstream tasks via Low-Rank Adaptation, by Edward Hu, Yelong Shen, Phillip Wallis, Zeyu Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. The paper was published at ICLR 2022.

Inspired by the work of [Aghajanyan et al.], the authors hypothesize that the updates to the weights have a low "intrinsic rank" during adaptation.
Based on the hypothesis, the idea of LoRA is very simple: we impose a restriction on the rank of the matrices for the parameter increment by writing it as $\Delta W = BA$, where $\Delta W \in \mathbb{R}^{d \times k}$ and $B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}$ for some $r$ which is chosen to be much smaller than $d$ and $k$.
In other words, the rank of the increament is at most $r$.
We freeze the initial parameters $W_0 \in \mathbb{R}^{d \times k}$ during training, and the modified forward pass (after training) would be

$$
h = W_0 x + \Delta W x = W_0 x + BA x.
$$

The matrix $A$ is initialized along Gaussian distribution, and the matrix $B$ is initialized as a zero matrix.
Figure 1 gives a description of LoRA, where the blue pre-trained weights are freezed and the orange matrices are new parameters to be trained.
LoRA can be regarded as a generalization of full fine-tuning, as the full fine-tuning can be thought as updating parameters without any rank restrictions.

<p align="center">
<img src="/assets/images/lora-figure1.png">
</p>

## Applying LoRA to transformers

In theory, LoRA can be applied to any subset of matrices of any neural networks, but this paper concentrates on the transformer-based LLMs.
Especially, there are four weight matrices in self-attension module (correspond to query, key, value, and output transforms) and two in the MLP (feed-forward) module, and the authors limit their strudy to only adapting the attention weights (and freeze MLP modules, LayerNorms, and biases).
The following tables (Table 2, 3, 4) show the comparison results of LoRA with other baselines, including full fine-tuning (**FT**), bias-only (**BitFit**, [Zaken et al.]), prefix-embedding tuning (**PreEmbed**, [Li and Liang]), prefix-layer tuning (**PreLayer**), and adapter tunings (**Adapter**, [Houlsby et al.], [Lin et al.], [Pfeiffer et al.], [Rücklé et al.]), applied to RoBERTa, DeBERTa, GPT-2, and GPT-3.


<p align="center">
<img src="/assets/images/lora-table2.png">
</p>

<p align="center">
<img src="/assets/images/lora-table3.png">
</p>

<p align="center">
<img src="/assets/images/lora-table4.png">
</p>






## Post works and conclusion

*References*:

[Houlsby et al.] *Parameter-Efficient Transfer Learning for NLP*,arXiv:1902.00751 (2019)

[Lin et al.] *Exploring versatile generative language model via parameter-efficient transfer learning*, Finings of the Association for Computational Linguistics: EMNLP (2020)

[Pfeiffer et al.] *Adapter-fusion: Non-distructive task composition for transfer learning* (2021)

[Zaken et al.] *Bitfit: Simple parameter-efficient fine-tuning for transformer-based mask language-models* (2021)

[Rücklé et al.] *Know that you don't know: Unanswerable questions for squad*, CoRR, abs/1806.03822 (2018)

[Aghajanyan et al.] *Intrinsic Dimensiontality Explains the Effectiveness of Language Model Fine-Tuning*, arXiv:2012.13255 (2020) 

[Li and Liang] *Prefix-Tuning: Optimizing Continuous Prompts for Generation*, arXiv:2101.00190 (2021)

